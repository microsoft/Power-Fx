{
  "swagger": "2.0",
  "info": {
    "title": "Azure OpenAI",
    "description": "Easily integrate Azure OpenAI's cutting-edge artificial intelligence capabilities into your workflows",
    "version": "1.0",
    "contact": {
      "name": "Microsoft",
      "url": "https://support.microsoft.com"
    }
  },
  "host": "azure-open-ai-resource-name.openai.azure.com",
  "basePath": "/openai/",
  "schemes": [
    "https"
  ],
  "consumes": [
    "application/json"
  ],
  "produces": [
    "application/json"
  ],
  "paths": {
    "/deployments/{deployment-id}/completions": {
      "post": {
        "operationId": "Completions_Create",
        "summary": "Creates a completion for the provided prompt, parameters and chosen model",
        "description": "Creates a completion for the provided prompt, parameters and chosen model",
        "x-ms-visibility": "advanced",
        "parameters": [
          {
            "$ref": "#/parameters/DeploymentId"
          },
          {
            "$ref": "#/parameters/ApiVersion"
          },
          {
            "in": "body",
            "name": "requestBody",
            "required": true,
            "schema": {
              "type": "object",
              "required": [
                "prompt"
              ],
              "properties": {
                "prompt": {
                  "description": "The prompt(s) to generate completions for, encoded as a string or array of strings. Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document. Maximum allowed size of string list is 2048.",
                  "type": "array",
                  "minItems": 1,
                  "maxItems": 2048,
                  "items": {
                    "type": "string",
                    "default": "",
                    "example": "This is a test.",
                    "description": "Array size minimum of 1 and maximum of 2048"
                  }
                },
                "max_tokens": {
                  "description": "The token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096). Has minimum of 0.",
                  "type": "integer",
                  "default": 250,
                  "example": 250
                },
                "temperature": {
                  "description": "What sampling temperature to use. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. We generally recommend altering this or top_p but not both.",
                  "type": "number",
                  "default": 0.9,
                  "example": 0.9
                },
                "top_p": {
                  "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.",
                  "type": "number",
                  "default": 0.75,
                  "example": 0.75
                },
                "logit_bias": {
                  "description": "Defaults to null. Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {\"50256\" &#58; -100} to prevent the <|endoftext|> token from being generated.",
                  "type": "object"
                },
                "user": {
                  "description": "A unique identifier representing your end-user, which can help monitoring and detecting abuse",
                  "type": "string"
                },
                "n": {
                  "description": "How many completions to generate for each prompt. Minimum of 1 and maximum of 128 allowed. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.",
                  "type": "integer",
                  "default": 1,
                  "example": 1
                },
                "stream": {
                  "description": "Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.",
                  "type": "boolean",
                  "default": false
                },
                "logprobs": {
                  "description": "Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response. Minimum of 0 and maximum of 5 allowed.",
                  "type": "integer"
                },
                "model": {
                  "type": "string",
                  "example": "davinci",
                  "description": "ID of the model to use. You can use the Models_List operation to see all of your available models, or see our Models_Get overview for descriptions of them."
                },
                "suffix": {
                  "type": "string",
                  "description": "The suffix that comes after a completion of inserted text."
                },
                "echo": {
                  "description": "Echo back the prompt in addition to the completion",
                  "type": "boolean"
                },
                "stop": {
                  "description": "Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.",
                  "type": "array",
                  "minItems": 1,
                  "maxItems": 4,
                  "items": {
                    "type": "string",
                    "default": "<|endoftext|>",
                    "example": "\n"
                  },
                  "default": null
                },
                "completion_config": {
                  "type": "string"
                },
                "cache_level": {
                  "description": "can be used to disable any server-side caching, 0=no cache, 1=prompt prefix enabled, 2=full cache",
                  "type": "integer"
                },
                "presence_penalty": {
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
                  "type": "number",
                  "default": 0,
                  "minimum": -2,
                  "maximum": 2
                },
                "frequency_penalty": {
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
                  "type": "number",
                  "default": 0,
                  "minimum": -2,
                  "maximum": 2
                },
                "best_of": {
                  "description": "Generates best_of completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed. When used with n, best_of controls the number of candidate completions and n specifies how many to return \u2013 best_of must be greater than n. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. Has maximum value of 128.",
                  "type": "integer"
                }
              }
            }
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "schema": {
              "type": "object",
              "properties": {
                "id": {
                  "type": "string"
                },
                "object": {
                  "type": "string"
                },
                "created": {
                  "type": "integer",
                  "format": "unixtime"
                },
                "model": {
                  "type": "string"
                },
                "choices": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "text": {
                        "type": "string"
                      },
                      "index": {
                        "type": "integer"
                      },
                      "logprobs": {
                        "type": "object",
                        "properties": {
                          "tokens": {
                            "type": "array",
                            "items": {
                              "type": "string"
                            }
                          },
                          "token_logprobs": {
                            "type": "array",
                            "items": {
                              "type": "number"
                            }
                          },
                          "top_logprobs": {
                            "type": "array",
                            "items": {
                              "type": "object",
                              "additionalProperties": {
                                "type": "number"
                              }
                            }
                          },
                          "text_offset": {
                            "type": "array",
                            "items": {
                              "type": "integer"
                            }
                          }
                        }
                      },
                      "finish_reason": {
                        "type": "string"
                      }
                    }
                  }
                },
                "usage": {
                  "type": "object",
                  "properties": {
                    "prompt_tokens": {
                      "type": "integer"
                    },
                    "completion_tokens": {
                      "type": "integer"
                    },
                    "total_tokens": {
                      "type": "integer"
                    }
                  },
                  "required": [
                    "prompt_tokens",
                    "completion_tokens",
                    "total_tokens"
                  ]
                }
              },
              "required": [
                "id",
                "object",
                "created",
                "model",
                "choices"
              ]
            },
            "headers": {
              "apim-request-id": {
                "description": "Request ID for troubleshooting purposes",
                "type": "string"
              }
            }
          },
          "default": {
            "description": "Service unavailable",
            "headers": {
              "apim-request-id": {
                "description": "Request ID for troubleshooting purposes",
                "type": "string"
              }
            }
          }
        }
      }
    },
    "/deployments/{deployment-id}/chat/completions": {
      "post": {
        "operationId": "ChatCompletions_Create",
        "summary": "Creates a completion for the chat message",
        "description": "Creates a completion for the chat message",
        "x-ms-visibility": "advanced",
        "parameters": [
          {
            "$ref": "#/parameters/DeploymentId"
          },
          {
            "$ref": "#/parameters/ApiVersion"
          },
          {
            "in": "body",
            "name": "requestBody",
            "required": true,
            "schema": {
              "type": "object",
              "required": [
                "messages"
              ],
              "properties": {
                "messages": {
                  "description": "The messages to generate chat completions for, in the chat format.",
                  "type": "array",
                  "minItems": 1,
                  "items": {
                    "type": "object",
                    "properties": {
                      "role": {
                        "type": "string",
                        "enum": [
                          "system",
                          "user",
                          "assistant"
                        ],
                        "description": "The role of the author of this message."
                      },
                      "content": {
                        "type": "string",
                        "description": "The contents of the message"
                      },
                      "name": {
                        "type": "string",
                        "description": "The name of the user in a multi-user chat"
                      }
                    },
                    "required": [
                      "role",
                      "content"
                    ]
                  }
                },
                "temperature": {
                  "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or `top_p` but not both.",
                  "type": "number",
                  "minimum": 0,
                  "maximum": 2,
                  "default": 1,
                  "example": 1
                },
                "top_p": {
                  "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or `temperature` but not both.",
                  "type": "number",
                  "minimum": 0,
                  "maximum": 1,
                  "default": 1,
                  "example": 1
                },
                "n": {
                  "description": "How many chat completion choices to generate for each input message.",
                  "type": "integer",
                  "minimum": 1,
                  "maximum": 128,
                  "default": 1,
                  "example": 1
                },
                "stream": {
                  "description": "If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.",
                  "type": "boolean",
                  "default": false
                },
                "stop": {
                  "description": "Up to 4 sequences where the API will stop generating further tokens.",
                  "type": "array",
                  "minItems": 1,
                  "maxItems": 4,
                  "items": {
                    "type": "string"
                  },
                  "default": null
                },
                "max_tokens": {
                  "description": "The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).",
                  "type": "integer",
                  "default": 250
                },
                "presence_penalty": {
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
                  "type": "number",
                  "default": 0,
                  "minimum": -2,
                  "maximum": 2
                },
                "frequency_penalty": {
                  "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
                  "type": "number",
                  "default": 0,
                  "minimum": -2,
                  "maximum": 2
                },
                "logit_bias": {
                  "description": "Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.",
                  "type": "object"
                },
                "user": {
                  "description": "A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse.",
                  "type": "string",
                  "example": "user-1234"
                }
              },
              "example": {
                "model": "gpt-35-turbo",
                "messages": [
                  {
                    "role": "user",
                    "content": "Hello!"
                  }
                ]
              }
            }
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "schema": {
              "type": "object",
              "properties": {
                "id": {
                  "type": "string"
                },
                "object": {
                  "type": "string"
                },
                "created": {
                  "type": "integer",
                  "format": "unixtime"
                },
                "model": {
                  "type": "string"
                },
                "choices": {
                  "type": "array",
                  "items": {
                    "type": "object",
                    "properties": {
                      "index": {
                        "type": "integer"
                      },
                      "message": {
                        "type": "object",
                        "properties": {
                          "role": {
                            "type": "string",
                            "enum": [
                              "system",
                              "user",
                              "assistant"
                            ],
                            "description": "The role of the author of this message."
                          },
                          "content": {
                            "type": "string",
                            "description": "The contents of the message"
                          }
                        },
                        "required": [
                          "role",
                          "content"
                        ]
                      },
                      "finish_reason": {
                        "type": "string"
                      }
                    }
                  }
                },
                "usage": {
                  "type": "object",
                  "properties": {
                    "prompt_tokens": {
                      "type": "integer"
                    },
                    "completion_tokens": {
                      "type": "integer"
                    },
                    "total_tokens": {
                      "type": "integer"
                    }
                  },
                  "required": [
                    "prompt_tokens",
                    "completion_tokens",
                    "total_tokens"
                  ]
                }
              },
              "required": [
                "id",
                "object",
                "created",
                "model",
                "choices"
              ],
              "example": {
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "model": "gpt-35-turbo",
                "created": 1677652288,
                "choices": [
                  {
                    "index": 0,
                    "message": {
                      "role": "assistant",
                      "content": "\n\nHello there, how may I assist you today?"
                    },
                    "finish_reason": "stop"
                  }
                ],
                "usage": {
                  "prompt_tokens": 9,
                  "completion_tokens": 12,
                  "total_tokens": 21
                }
              }
            },
            "headers": {
              "apim-request-id": {
                "description": "Request ID for troubleshooting purposes",
                "type": "string"
              }
            }
          },
          "default": {
            "description": "Service unavailable",
            "headers": {
              "apim-request-id": {
                "description": "Request ID for troubleshooting purposes",
                "type": "string"
              }
            }
          }
        }
      }
    },
    "/deployments/{deployment-id}/extensions/chat/completions": {
      "post": {
        "operationId": "ExtensionsChatCompletions_Create",
        "summary": "Using extensions to create a completion for chat messages",
        "description": "Using extensions to create a completion for chat messages",
        "x-ms-visibility": "advanced",
        "parameters": [
          {
            "$ref": "#/parameters/DeploymentId"
          },
          {
            "$ref": "#/parameters/DeploymentIdQueryParam"
          },
          {
            "$ref": "#/parameters/ApiVersion"
          },
          {
            "in": "body",
            "name": "requestBody",
            "required": true,
            "schema": {
              "$ref": "#/definitions/ExtensionsChatCompletionsRequest"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "OK",
            "schema": {
              "$ref": "#/definitions/ExtensionsChatCompletionsResponse"
            },
            "headers": {
              "apim-request-id": {
                "description": "Request ID for troubleshooting purposes",
                "type": "string"
              }
            }
          },
          "default": {
            "description": "Service unavailable",
            "headers": {
              "apim-request-id": {
                "description": "Request ID for troubleshooting purposes",
                "type": "string"
              }
            }
          }
        }
      }
    }
  },
  "definitions": {
    "errorResponse": {
      "type": "object",
      "properties": {
        "error": {
          "type": "object",
          "properties": {
            "code": {
              "type": "string"
            },
            "message": {
              "type": "string"
            },
            "param": {
              "type": "string"
            },
            "type": {
              "type": "string"
            }
          }
        }
      }
    },
    "ExtensionsChatCompletionsRequest": {
      "type": "object",
      "description": "Request for the chat completions using extensions",
      "required": [
        "messages"
      ],
      "properties": {
        "messages": {
          "type": "array",
          "items": {
            "$ref": "#/definitions/Message"
          }
        },
        "dataSources": {
          "type": "array",
          "description": "The data sources to be used for the Azure OpenAI on your data feature.",
          "items": {
            "$ref": "#/definitions/DataSource"
          }
        },
        "temperature": {
          "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or `top_p` but not both.",
          "type": "number",
          "minimum": 0,
          "maximum": 2,
          "default": 1,
          "example": 1
        },
        "top_p": {
          "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or `temperature` but not both.",
          "type": "number",
          "minimum": 0,
          "maximum": 1,
          "default": 1,
          "example": 1
        },
        "stream": {
          "description": "If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.",
          "type": "boolean",
          "default": false
        },
        "stop": {
          "description": "Array minimum size of 1 and maximum of 4",
          "type": "array",
          "items": {
            "type": "string"
          },
          "minItems": 1,
          "maxItems": 4,
          "default": null
        },
        "max_tokens": {
          "description": "The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).",
          "type": "integer",
          "default": 250
        },
        "presence_penalty": {
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
          "type": "number",
          "default": 0,
          "minimum": -2,
          "maximum": 2
        },
        "frequency_penalty": {
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
          "type": "number",
          "default": 0,
          "minimum": -2,
          "maximum": 2
        },
        "logit_bias": {
          "description": "Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.",
          "type": "object"
        },
        "user": {
          "description": "A unique identifier representing your end-user, which can help Azure OpenAI to monitor and detect abuse.",
          "type": "string",
          "example": "user-1234"
        }
      },
      "example": {
        "dataSources": [
          {
            "type": "AzureCognitiveSearch",
            "parameters": {
              "endpoint": "https://mysearchexample.search.windows.net",
              "key": "***(admin key)",
              "indexName": "my-chunk-index",
              "fieldsMapping": {
                "titleField": "productName",
                "urlField": "productUrl",
                "filepathField": "productFilePath",
                "contentFields": [
                  "productDescription"
                ],
                "contentFieldsSeparator": "\n"
              },
              "topNDocuments": 5,
              "queryType": "semantic",
              "semanticConfiguration": "defaultConfiguration",
              "inScope": true,
              "roleInformation": "roleInformation"
            }
          }
        ],
        "messages": [
          {
            "role": "user",
            "content": "Where can I find a hiking place in Seattle?"
          }
        ],
        "temperature": 0.9
      }
    },
    "DataSource": {
      "type": "object",
      "description": "The data source to be used for the Azure OpenAI on your data feature.",
      "properties": {
        "type": {
          "type": "string",
          "description": "The data source type."
        },
        "parameters": {
          "type": "object",
          "description": "The parameters to be used for the data source in runtime.",
          "additionalProperties": true
        }
      },
      "required": [
        "type"
      ]
    },
    "Message": {
      "type": "object",
      "description": "A chat message.",
      "properties": {
        "index": {
          "type": "integer",
          "description": "The index of the message in the conversation."
        },
        "role": {
          "type": "string",
          "enum": [
            "system",
            "user",
            "assistant",
            "tool"
          ],
          "description": "The role of the author of this message."
        },
        "recipient": {
          "type": "string",
          "example": "Contoso.productsUsingGET",
          "description": "The recipient of the message in the format of <namespace>.<operation>. Present if and only if the recipient is tool."
        },
        "content": {
          "type": "string",
          "description": "The contents of the message"
        },
        "end_turn": {
          "type": "boolean",
          "description": "Whether the message ends the turn."
        }
      },
      "required": [
        "role",
        "content"
      ]
    },
    "ExtensionsChatCompletionsResponse": {
      "type": "object",
      "description": "The response of the extensions chat completions.",
      "required": [
        "id",
        "object",
        "created",
        "model"
      ],
      "properties": {
        "id": {
          "type": "string"
        },
        "object": {
          "type": "string"
        },
        "created": {
          "type": "integer",
          "format": "unixtime"
        },
        "model": {
          "type": "string"
        },
        "prompt_filter_results": {
          "$ref": "#/definitions/promptFilterResults"
        },
        "usage": {
          "type": "object",
          "properties": {
            "prompt_tokens": {
              "type": "integer"
            },
            "completion_tokens": {
              "type": "integer"
            },
            "total_tokens": {
              "type": "integer"
            }
          },
          "required": [
            "prompt_tokens",
            "completion_tokens",
            "total_tokens"
          ]
        },
        "choices": {
          "type": "array",
          "items": {
            "$ref": "#/definitions/ExtensionsChatCompletionChoice"
          }
        }
      },
      "example": {
        "id": "1",
        "object": "extensions.chat.completion",
        "created": 1679201802,
        "model": "gpt-3.5-turbo-0301",
        "choices": [
          {
            "index": 0,
            "finish_reason": "stop",
            "messages": [
              {
                "index": 0,
                "role": "assistant",
                "recipient": "MySearch.Query",
                "content": "{\"query\":\"hiking places in Seattle\"}"
              },
              {
                "index": 1,
                "role": "tool",
                "content": "{\"citations\":[{\"id\":1,\"name\":\"ContosoTraveler.pdf\",\"link\":\"https://www.example.com/ContosoTraveler.pdf\",\"content\":\"This is the content of the citation 1\"},{\"id\":2,\"name\":\"WestCoastTraveler.html\",\"link\":\"https://www.example.com/WestCoastTraveler.html\",\"content\":\"This is the content of the citation 2\"},{\"id\":3,\"name\":\"Citation3\",\"link\":\"https://www.example.com/3\",\"content\":\"This is the content of the citation 3\"}],\"intent\":\"hiking place in seattle\"}"
              },
              {
                "index": 2,
                "role": "assistant",
                "content": "Seattle is a great place for hiking! Here are some of the best hiking places in Seattle according to Contoso Traveler [^1^] and West Coast Traveler, Snow Lake, Mount Si, and Mount Tenerife [^2^]. I hope this helps! Let me know if you need more information."
              }
            ]
          }
        ]
      }
    },
    "ExtensionsChatCompletionChoice": {
      "type": "object",
      "properties": {
        "index": {
          "type": "integer"
        },
        "finish_reason": {
          "type": "string"
        },
        "content_filter_results": {
          "$ref": "#/definitions/contentFilterResults"
        },
        "messages": {
          "type": "array",
          "description": "The list of messages returned by the service.",
          "items": {
            "$ref": "#/definitions/Message"
          }
        }
      }
    },
    "contentFilterResult": {
      "type": "object",
      "properties": {
        "severity": {
          "type": "string",
          "enum": [
            "safe",
            "low",
            "medium",
            "high"
          ],
          "x-ms-enum": {
            "name": "ContentFilterSeverity",
            "modelAsString": true,
            "values": [
              {
                "value": "safe",
                "description": "General content or related content in generic or non-harmful contexts."
              },
              {
                "value": "low",
                "description": "Harmful content at a low intensity and risk level."
              },
              {
                "value": "medium",
                "description": "Harmful content at a medium intensity and risk level."
              },
              {
                "value": "high",
                "description": "Harmful content at a high intensity and risk level."
              }
            ]
          }
        },
        "filtered": {
          "type": "boolean"
        }
      },
      "required": [
        "severity",
        "filtered"
      ]
    },
    "contentFilterResults": {
      "type": "object",
      "description": "Information about the content filtering category (hate, sexual, violence, self_harm), if it has been detected, as well as the severity level (very_low, low, medium, high-scale that determines the intensity and risk level of harmful content) and if it has been filtered or not.",
      "properties": {
        "sexual": {
          "$ref": "#/definitions/contentFilterResult"
        },
        "violence": {
          "$ref": "#/definitions/contentFilterResult"
        },
        "hate": {
          "$ref": "#/definitions/contentFilterResult"
        },
        "self_harm": {
          "$ref": "#/definitions/contentFilterResult"
        },
        "error": {
          "$ref": "#/definitions/errorBase"
        }
      }
    },
    "promptFilterResult": {
      "type": "object",
      "description": "Content filtering results for a single prompt in the request.",
      "properties": {
        "prompt_index": {
          "type": "integer"
        },
        "content_filter_results": {
          "$ref": "#/definitions/contentFilterResults"
        }
      }
    },
    "promptFilterResults": {
      "type": "array",
      "description": "Content filtering results for zero or more prompts in the request. In a streaming request, results for different prompts may arrive at different times or in different orders.",
      "items": {
        "$ref": "#/definitions/promptFilterResult"
      }
    },
    "errorBase": {
      "type": "object",
      "properties": {
        "code": {
          "type": "string"
        },
        "message": {
          "type": "string"
        }
      }
    }
  },
  "parameters": {
    "DeploymentId": {
      "in": "path",
      "name": "deployment-id",
      "required": true,
      "type": "string",
      "description": "Deployment ID of the deployed model",
      "x-ms-summary": "Deployment ID of the deployed model"
    },
    "DeploymentIdQueryParam": {
      "in": "query",
      "name": "deploymentId",
      "required": true,
      "type": "string",
      "description": "Confirm Deployment ID of the deployed model",
      "x-ms-summary": "Confirm Deployment ID of the deployed model"
    },
    "ApiVersion": {
      "in": "query",
      "name": "api-version",
      "required": true,
      "type": "string",
      "description": "API version",
      "x-ms-summary": "API version"
    }
  },
  "responses": {},
  "securityDefinitions": {
    "custom": {
      "type": "basic"
    }
  },
  "security": [],
  "tags": [],
  "x-ms-connector-metadata": [
    {
      "propertyName": "Website",
      "propertyValue": "https://azure.microsoft.com/en-us/products/cognitive-services/openai-service"
    },
    {
      "propertyName": "Privacy policy",
      "propertyValue": "https://learn.microsoft.com/en-us/legal/cognitive-services/openai/data-privacy"
    },
    {
      "propertyName": "Categories",
      "propertyValue": "AI;Business Intelligence"
    }
  ]
}